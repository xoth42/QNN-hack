{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid CNN + Quantum Neural Network for CIFAR-10\n",
    "Implements a density QNN architecture based on the paper's framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import pennylane as qml\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "Load AWS configuration from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current AWS Configuration:\n",
      "Region: us-east-1\n",
      "Access Key ID: AKIA************ZMTY\n",
      "Secret Access Key: RXLx********************************i0yI\n",
      "Braket Device: arn:aws:braket:::device/quantum-simulator/amazon/sv1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Function to safely get and mask sensitive environment variables\n",
    "def get_masked_env(var_name):\n",
    "    value = os.getenv(var_name, '')\n",
    "    if value and var_name in ['AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY']:\n",
    "        return value[:4] + '*' * (len(value) - 8) + value[-4:]\n",
    "    return value\n",
    "\n",
    "# Display current configuration\n",
    "print(\"Current AWS Configuration:\")\n",
    "print(f\"Region: {get_masked_env('AWS_DEFAULT_REGION')}\")\n",
    "print(f\"Access Key ID: {get_masked_env('AWS_ACCESS_KEY_ID')}\")\n",
    "print(f\"Secret Access Key: {get_masked_env('AWS_SECRET_ACCESS_KEY')}\")\n",
    "print(f\"Braket Device: {get_masked_env('BRAKET_DEVICE')}\")\n",
    "\n",
    "# Function to update AWS configuration\n",
    "def update_aws_config(region=None, access_key=None, secret_key=None, device_arn=None):\n",
    "    if region:\n",
    "        os.environ['AWS_DEFAULT_REGION'] = region\n",
    "    if access_key:\n",
    "        os.environ['AWS_ACCESS_KEY_ID'] = access_key\n",
    "    if secret_key:\n",
    "        os.environ['AWS_SECRET_ACCESS_KEY'] = secret_key\n",
    "    if device_arn:\n",
    "        os.environ['BRAKET_DEVICE'] = device_arn\n",
    "    \n",
    "    clear_output()\n",
    "    print(\"Updated AWS Configuration:\")\n",
    "    print(f\"Region: {get_masked_env('AWS_DEFAULT_REGION')}\")\n",
    "    print(f\"Access Key ID: {get_masked_env('AWS_ACCESS_KEY_ID')}\")\n",
    "    print(f\"Secret Access Key: {get_masked_env('AWS_SECRET_ACCESS_KEY')}\")\n",
    "    print(f\"Braket Device: {get_masked_env('BRAKET_DEVICE')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Loaded batch: torch.Size([4, 3, 32, 32])\n",
      "Loaded batch: torch.Size([4, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR-10 sample\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True)\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "print(f\"Loaded batch: {images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amazon Braket SV1 simulator device\n",
    "dev = qml.device(\n",
    "    \"braket.aws.qubit\",\n",
    "    device_arn=os.getenv('BRAKET_DEVICE'),\n",
    "    wires=4,\n",
    "    shots=100  # Set number of shots here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density QNN: Sub-unitary quantum circuit (RBS-based)\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def quantum_sub_circuit(inputs, weights):\n",
    "    # Data encoding\n",
    "    for i in range(4):\n",
    "        qml.RY(inputs[i], wires=i)\n",
    "    \n",
    "    # Parameterized RBS-inspired gates\n",
    "    for i in range(4):\n",
    "        qml.RZ(weights[i], wires=i)\n",
    "    \n",
    "    # Entanglement (CNOT ladder)\n",
    "    for i in range(3):\n",
    "        qml.CNOT(wires=[i, i+1])\n",
    "    \n",
    "    # Second rotation layer\n",
    "    for i in range(4):\n",
    "        qml.RY(weights[i+4], wires=i)\n",
    "    \n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid CNN + Density Quantum Model\n",
    "class HybridDensityQNN(nn.Module):\n",
    "    def __init__(self, num_sub_unitaries=2):\n",
    "        super(HybridDensityQNN, self).__init__()\n",
    "        \n",
    "        # CNN feature extractor\n",
    "        self.conv1 = nn.Conv2d(3, 8, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(8, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 4)\n",
    "        \n",
    "        # Density QNN: K sub-unitaries with independent parameters\n",
    "        self.K = num_sub_unitaries\n",
    "        self.quantum_layers = nn.ModuleList([\n",
    "            qml.qnn.TorchLayer(quantum_sub_circuit, {\"weights\": (8,)})\n",
    "            for _ in range(self.K)\n",
    "        ])\n",
    "        \n",
    "        # Trainable mixing coefficients α_k (ensure sum to 1 via softmax)\n",
    "        self.alpha = nn.Parameter(torch.ones(self.K))\n",
    "        \n",
    "        # Final classifier\n",
    "        self.fc2 = nn.Linear(4, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # CNN feature extraction\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        \n",
    "        # Density QNN: weighted sum of sub-unitaries\n",
    "        alpha_norm = torch.softmax(self.alpha, dim=0)\n",
    "        quantum_out = sum(alpha_norm[k] * self.quantum_layers[k](x) for k in range(self.K))\n",
    "        \n",
    "        # Classification\n",
    "        out = self.fc2(quantum_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 3, 32, 32])\n",
      "Output shape: torch.Size([4, 10])\n",
      "Mixing coefficients α: tensor([0.5000, 0.5000])\n",
      "\n",
      "Output logits:\n",
      "tensor([[ 0.2526, -0.3160,  0.0282,  0.6928,  0.0157, -0.7796, -0.5577,  0.0707,\n",
      "          0.1227, -0.4648],\n",
      "        [ 0.2553, -0.3378,  0.0234,  0.6888,  0.0476, -0.7318, -0.5319,  0.0629,\n",
      "          0.0822, -0.4633],\n",
      "        [ 0.2495, -0.3601,  0.0563,  0.6795, -0.0025, -0.7777, -0.5845,  0.0661,\n",
      "          0.1195, -0.4718],\n",
      "        [ 0.2814, -0.3382,  0.0272,  0.7245,  0.0389, -0.7296, -0.5493,  0.0232,\n",
      "          0.0646, -0.4749]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Initialize and test forward pass\n",
    "model = HybridDensityQNN(num_sub_unitaries=2)\n",
    "output = model(images)\n",
    "\n",
    "print(f\"Input shape: {images.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Mixing coefficients α: {torch.softmax(model.alpha, dim=0).detach()}\")\n",
    "print(f\"\\nOutput logits:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Analysis\n",
    "\n",
    "Let's analyze the model's output and performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FAIR MODEL COMPARISON: CNN vs Hybrid QNN\n",
      "============================================================\n",
      "\n",
      "Model Architecture Comparison:\n",
      "------------------------------------------------------------\n",
      "Pure CNN Parameters:        5,498\n",
      "Hybrid QNN Parameters:      5,496\n",
      "Parameter Difference:       2\n",
      "\n",
      "------------------------------------------------------------\n",
      "PREDICTION COMPARISON (Untrained Models)\n",
      "------------------------------------------------------------\n",
      "Image    True       Pure CNN     Hybrid QNN   Match?  \n",
      "------------------------------------------------------------\n",
      "1        plane      plane        cat          ✗       \n",
      "2        horse      plane        cat          ✗       \n",
      "3        dog        plane        cat          ✗       \n",
      "4        horse      plane        cat          ✗       \n",
      "------------------------------------------------------------\n",
      "Prediction Agreement: 0/4 (0%)\n",
      "\n",
      "------------------------------------------------------------\n",
      "CONFIDENCE SCORES\n",
      "------------------------------------------------------------\n",
      "Image    CNN Conf        Hybrid Conf     Difference  \n",
      "------------------------------------------------------------\n",
      "1         15.29%          20.45%          +5.16%\n",
      "2         15.31%          20.37%          +5.06%\n",
      "3         15.46%          19.93%          +4.48%\n",
      "4         15.29%          19.90%          +4.61%\n",
      "------------------------------------------------------------\n",
      "Avg CNN Confidence:    15.33%\n",
      "Avg Hybrid Confidence: 20.16%\n",
      "\n",
      "------------------------------------------------------------\n",
      "FEATURE SPACE ANALYSIS\n",
      "------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------\n",
      "PREDICTION COMPARISON (Untrained Models)\n",
      "------------------------------------------------------------\n",
      "Image    True       Pure CNN     Hybrid QNN   Match?  \n",
      "------------------------------------------------------------\n",
      "1        plane      plane        cat          ✗       \n",
      "2        horse      plane        cat          ✗       \n",
      "3        dog        plane        cat          ✗       \n",
      "4        horse      plane        cat          ✗       \n",
      "------------------------------------------------------------\n",
      "Prediction Agreement: 0/4 (0%)\n",
      "\n",
      "------------------------------------------------------------\n",
      "CONFIDENCE SCORES\n",
      "------------------------------------------------------------\n",
      "Image    CNN Conf        Hybrid Conf     Difference  \n",
      "------------------------------------------------------------\n",
      "1         15.29%          20.45%          +5.16%\n",
      "2         15.31%          20.37%          +5.06%\n",
      "3         15.46%          19.93%          +4.48%\n",
      "4         15.29%          19.90%          +4.61%\n",
      "------------------------------------------------------------\n",
      "Avg CNN Confidence:    15.33%\n",
      "Avg Hybrid Confidence: 20.16%\n",
      "\n",
      "------------------------------------------------------------\n",
      "FEATURE SPACE ANALYSIS\n",
      "------------------------------------------------------------\n",
      "\n",
      "Pure CNN Feature Layer:\n",
      "  Range: [0.000, 0.454]\n",
      "  Mean:  0.197 ± 0.154\n",
      "  Sparsity: 25.0% zeros\n",
      "\n",
      "Hybrid Quantum Layer:\n",
      "  Range: [-0.980, 0.030]\n",
      "  Mean:  -0.470 ± 0.434\n",
      "  Mixing α: [0.5 0.5]\n",
      "\n",
      "============================================================\n",
      "NOTE: Models are untrained - performance will improve after training\n",
      "============================================================\n",
      "\n",
      "Pure CNN Feature Layer:\n",
      "  Range: [0.000, 0.454]\n",
      "  Mean:  0.197 ± 0.154\n",
      "  Sparsity: 25.0% zeros\n",
      "\n",
      "Hybrid Quantum Layer:\n",
      "  Range: [-0.980, 0.030]\n",
      "  Mean:  -0.470 ± 0.434\n",
      "  Mixing α: [0.5 0.5]\n",
      "\n",
      "============================================================\n",
      "NOTE: Models are untrained - performance will improve after training\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Create Fair Comparison Models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FAIR MODEL COMPARISON: CNN vs Hybrid QNN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define pure CNN baseline (same capacity as hybrid)\n",
    "class PureCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PureCNN, self).__init__()\n",
    "        # Same CNN feature extractor\n",
    "        self.conv1 = nn.Conv2d(3, 8, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(8, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 4)\n",
    "        \n",
    "        # Extra classical layer to match quantum layer capacity\n",
    "        # Quantum has 2 sub-unitaries * 8 params each = 16 params\n",
    "        # Plus 4->10 final layer\n",
    "        self.fc_quantum_equiv = nn.Linear(4, 4)  # Replaces quantum layer\n",
    "        \n",
    "        # Final classifier\n",
    "        self.fc2 = nn.Linear(4, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.relu(self.fc_quantum_equiv(x))  # Classical \"quantum\" layer\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize both models\n",
    "pure_cnn = PureCNN()\n",
    "hybrid_qnn = model  # Use existing model\n",
    "\n",
    "# Count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "cnn_params = count_parameters(pure_cnn)\n",
    "hybrid_params = count_parameters(hybrid_qnn)\n",
    "\n",
    "print(\"\\nModel Architecture Comparison:\")\n",
    "print(\"-\"*60)\n",
    "print(f\"Pure CNN Parameters:        {cnn_params:,}\")\n",
    "print(f\"Hybrid QNN Parameters:      {hybrid_params:,}\")\n",
    "print(f\"Parameter Difference:       {abs(hybrid_params - cnn_params):,}\")\n",
    "\n",
    "# Run inference on same batch\n",
    "with torch.no_grad():\n",
    "    cnn_output = pure_cnn(images)\n",
    "    hybrid_output = hybrid_qnn(images)\n",
    "    \n",
    "    cnn_predictions = torch.argmax(cnn_output, dim=1)\n",
    "    hybrid_predictions = torch.argmax(hybrid_output, dim=1)\n",
    "    \n",
    "    cnn_probs = torch.softmax(cnn_output, dim=1)\n",
    "    hybrid_probs = torch.softmax(hybrid_output, dim=1)\n",
    "\n",
    "# CIFAR-10 classes\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', \n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Convert labels\n",
    "if isinstance(labels, list):\n",
    "    labels_tensor = torch.tensor([classes.index(label) for label in labels])\n",
    "else:\n",
    "    labels_tensor = labels\n",
    "\n",
    "# Display predictions\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"PREDICTION COMPARISON (Untrained Models)\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Image':<8} {'True':<10} {'Pure CNN':<12} {'Hybrid QNN':<12} {'Match?':<8}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "matches = 0\n",
    "for i in range(len(images)):\n",
    "    true_label = classes[labels_tensor[i]]\n",
    "    cnn_pred = classes[cnn_predictions[i]]\n",
    "    hybrid_pred = classes[hybrid_predictions[i]]\n",
    "    match = \"✓\" if cnn_pred == hybrid_pred else \"✗\"\n",
    "    \n",
    "    if cnn_pred == hybrid_pred:\n",
    "        matches += 1\n",
    "    \n",
    "    print(f\"{i+1:<8} {true_label:<10} {cnn_pred:<12} {hybrid_pred:<12} {match:<8}\")\n",
    "\n",
    "print(\"-\"*60)\n",
    "print(f\"Prediction Agreement: {matches}/{len(images)} ({matches/len(images)*100:.0f}%)\")\n",
    "\n",
    "# Confidence comparison\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"CONFIDENCE SCORES\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Image':<8} {'CNN Conf':<15} {'Hybrid Conf':<15} {'Difference':<12}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for i in range(len(images)):\n",
    "    cnn_conf = cnn_probs[i][cnn_predictions[i]] * 100\n",
    "    hybrid_conf = hybrid_probs[i][hybrid_predictions[i]] * 100\n",
    "    diff = hybrid_conf - cnn_conf\n",
    "    \n",
    "    print(f\"{i+1:<8} {cnn_conf:>6.2f}%{'':<8} {hybrid_conf:>6.2f}%{'':<8} {diff:>+6.2f}%\")\n",
    "\n",
    "print(\"-\"*60)\n",
    "print(f\"Avg CNN Confidence:    {cnn_probs.max(dim=1)[0].mean()*100:.2f}%\")\n",
    "print(f\"Avg Hybrid Confidence: {hybrid_probs.max(dim=1)[0].mean()*100:.2f}%\")\n",
    "\n",
    "# Feature space analysis\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"FEATURE SPACE ANALYSIS\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # CNN intermediate features\n",
    "    x = pure_cnn.pool(torch.relu(pure_cnn.conv1(images)))\n",
    "    x = pure_cnn.pool(torch.relu(pure_cnn.conv2(x)))\n",
    "    x = x.view(-1, 16 * 5 * 5)\n",
    "    cnn_pre_features = pure_cnn.fc1(x)\n",
    "    cnn_features = torch.relu(pure_cnn.fc_quantum_equiv(torch.tanh(cnn_pre_features)))\n",
    "    \n",
    "    # Hybrid quantum features\n",
    "    x = hybrid_qnn.pool(torch.relu(hybrid_qnn.conv1(images)))\n",
    "    x = hybrid_qnn.pool(torch.relu(hybrid_qnn.conv2(x)))\n",
    "    x = x.view(-1, 16 * 5 * 5)\n",
    "    hybrid_pre_features = hybrid_qnn.fc1(x)\n",
    "    \n",
    "    quantum_features = torch.tanh(hybrid_pre_features)\n",
    "    alpha_norm = torch.softmax(hybrid_qnn.alpha, dim=0)\n",
    "    \n",
    "    quantum_outputs = []\n",
    "    for sample in quantum_features:\n",
    "        circuit_outs = [layer(sample) for layer in hybrid_qnn.quantum_layers]\n",
    "        weighted_out = sum(alpha_norm[k] * circuit_outs[k] for k in range(hybrid_qnn.K))\n",
    "        quantum_outputs.append(weighted_out)\n",
    "    quantum_outputs = torch.stack(quantum_outputs)\n",
    "\n",
    "print(\"\\nPure CNN Feature Layer:\")\n",
    "print(f\"  Range: [{cnn_features.min():.3f}, {cnn_features.max():.3f}]\")\n",
    "print(f\"  Mean:  {cnn_features.mean():.3f} ± {cnn_features.std():.3f}\")\n",
    "print(f\"  Sparsity: {(cnn_features == 0).float().mean()*100:.1f}% zeros\")\n",
    "\n",
    "print(\"\\nHybrid Quantum Layer:\")\n",
    "print(f\"  Range: [{quantum_outputs.min():.3f}, {quantum_outputs.max():.3f}]\")\n",
    "print(f\"  Mean:  {quantum_outputs.mean():.3f} ± {quantum_outputs.std():.3f}\")\n",
    "print(f\"  Mixing α: {alpha_norm.detach().numpy()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NOTE: Models are untrained - performance will improve after training\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODEL PERFORMANCE COMPARISON\n",
      "============================================================\n",
      "\n",
      "------------------------------------------------------------\n",
      "BATCH PREDICTIONS (4 images)\n",
      "------------------------------------------------------------\n",
      "Image    True Label   CNN Only     Hybrid QNN  \n",
      "------------------------------------------------------------\n",
      "1        plane        deer         cat         \n",
      "2        horse        deer         cat         \n",
      "3        dog          deer         cat         \n",
      "4        horse        deer         cat         \n",
      "\n",
      "------------------------------------------------------------\n",
      "CONFIDENCE SCORES (Top prediction)\n",
      "------------------------------------------------------------\n",
      "Image 1: CNN=14.9%  |  Hybrid=20.1%\n",
      "Image 2: CNN=15.4%  |  Hybrid=20.4%\n",
      "Image 3: CNN=14.5%  |  Hybrid=20.1%\n",
      "Image 4: CNN=15.6%  |  Hybrid=20.7%\n",
      "\n",
      "------------------------------------------------------------\n",
      "FEATURE SPACE ANALYSIS\n",
      "------------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------------\n",
      "BATCH PREDICTIONS (4 images)\n",
      "------------------------------------------------------------\n",
      "Image    True Label   CNN Only     Hybrid QNN  \n",
      "------------------------------------------------------------\n",
      "1        plane        deer         cat         \n",
      "2        horse        deer         cat         \n",
      "3        dog          deer         cat         \n",
      "4        horse        deer         cat         \n",
      "\n",
      "------------------------------------------------------------\n",
      "CONFIDENCE SCORES (Top prediction)\n",
      "------------------------------------------------------------\n",
      "Image 1: CNN=14.9%  |  Hybrid=20.1%\n",
      "Image 2: CNN=15.4%  |  Hybrid=20.4%\n",
      "Image 3: CNN=14.5%  |  Hybrid=20.1%\n",
      "Image 4: CNN=15.6%  |  Hybrid=20.7%\n",
      "\n",
      "------------------------------------------------------------\n",
      "FEATURE SPACE ANALYSIS\n",
      "------------------------------------------------------------\n",
      "CNN Feature Statistics:\n",
      "  Range: [-0.144, 0.197]\n",
      "  Mean: 0.063, Std: 0.081\n",
      "\n",
      "Quantum Feature Statistics:\n",
      "  Range: [-0.990, 0.030]\n",
      "  Mean: -0.479, Std: 0.437\n",
      "\n",
      "Quantum Layer Info:\n",
      "  Sub-unitaries: 2\n",
      "  Mixing coefficients α: [0.5 0.5]\n",
      "  Qubit count: 4\n",
      "  Parameters per circuit: 8\n",
      "\n",
      "============================================================\n",
      "CNN Feature Statistics:\n",
      "  Range: [-0.144, 0.197]\n",
      "  Mean: 0.063, Std: 0.081\n",
      "\n",
      "Quantum Feature Statistics:\n",
      "  Range: [-0.990, 0.030]\n",
      "  Mean: -0.479, Std: 0.437\n",
      "\n",
      "Quantum Layer Info:\n",
      "  Sub-unitaries: 2\n",
      "  Mixing coefficients α: [0.5 0.5]\n",
      "  Qubit count: 4\n",
      "  Parameters per circuit: 8\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Simplified Performance Comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Test CNN-only path (without quantum layer)\n",
    "with torch.no_grad():\n",
    "    # Extract CNN features\n",
    "    x = model.pool(torch.relu(model.conv1(images)))\n",
    "    x = model.pool(torch.relu(model.conv2(x)))\n",
    "    x = x.view(-1, 16 * 5 * 5)\n",
    "    cnn_features = model.fc1(x)\n",
    "    \n",
    "    # CNN-only prediction (skip quantum, go straight to classifier)\n",
    "    cnn_only_output = model.fc2(cnn_features)\n",
    "    cnn_predictions = torch.argmax(cnn_only_output, dim=1)\n",
    "    \n",
    "    # Full hybrid model prediction\n",
    "    hybrid_output = model(images)\n",
    "    hybrid_predictions = torch.argmax(hybrid_output, dim=1)\n",
    "\n",
    "# CIFAR-10 classes\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', \n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Convert labels to tensor if needed\n",
    "if isinstance(labels, list):\n",
    "    labels_tensor = torch.tensor([classes.index(label) for label in labels])\n",
    "else:\n",
    "    labels_tensor = labels\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"BATCH PREDICTIONS (4 images)\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Image':<8} {'True Label':<12} {'CNN Only':<12} {'Hybrid QNN':<12}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for i in range(len(images)):\n",
    "    true_label = classes[labels_tensor[i]]\n",
    "    cnn_pred = classes[cnn_predictions[i]]\n",
    "    hybrid_pred = classes[hybrid_predictions[i]]\n",
    "    \n",
    "    print(f\"{i+1:<8} {true_label:<12} {cnn_pred:<12} {hybrid_pred:<12}\")\n",
    "\n",
    "# Compare confidence scores\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"CONFIDENCE SCORES (Top prediction)\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "cnn_probs = torch.softmax(cnn_only_output, dim=1)\n",
    "hybrid_probs = torch.softmax(hybrid_output, dim=1)\n",
    "\n",
    "for i in range(len(images)):\n",
    "    cnn_conf = cnn_probs[i][cnn_predictions[i]] * 100\n",
    "    hybrid_conf = hybrid_probs[i][hybrid_predictions[i]] * 100\n",
    "    \n",
    "    print(f\"Image {i+1}: CNN={cnn_conf:.1f}%  |  Hybrid={hybrid_conf:.1f}%\")\n",
    "\n",
    "# Feature space statistics\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"FEATURE SPACE ANALYSIS\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Get quantum features\n",
    "with torch.no_grad():\n",
    "    quantum_features = torch.tanh(cnn_features)\n",
    "    alpha_norm = torch.softmax(model.alpha, dim=0)\n",
    "    \n",
    "    # Process each sample individually\n",
    "    quantum_outputs = []\n",
    "    for sample in quantum_features:\n",
    "        circuit_outs = [layer(sample) for layer in model.quantum_layers]\n",
    "        weighted_out = sum(alpha_norm[k] * circuit_outs[k] for k in range(model.K))\n",
    "        quantum_outputs.append(weighted_out)\n",
    "    quantum_outputs = torch.stack(quantum_outputs)\n",
    "\n",
    "print(f\"CNN Feature Statistics:\")\n",
    "print(f\"  Range: [{cnn_features.min():.3f}, {cnn_features.max():.3f}]\")\n",
    "print(f\"  Mean: {cnn_features.mean():.3f}, Std: {cnn_features.std():.3f}\")\n",
    "\n",
    "print(f\"\\nQuantum Feature Statistics:\")\n",
    "print(f\"  Range: [{quantum_outputs.min():.3f}, {quantum_outputs.max():.3f}]\")\n",
    "print(f\"  Mean: {quantum_outputs.mean():.3f}, Std: {quantum_outputs.std():.3f}\")\n",
    "\n",
    "print(f\"\\nQuantum Layer Info:\")\n",
    "print(f\"  Sub-unitaries: {model.K}\")\n",
    "print(f\"  Mixing coefficients α: {alpha_norm.detach().numpy()}\")\n",
    "print(f\"  Qubit count: 4\")\n",
    "print(f\"  Parameters per circuit: 8\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
